I"hs<p>â€œAttention is all you needâ€(Vaswani et al. 2017).</p>

<p><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></p>

<p>NLPì—ì„œ ì‚¬ìš©ë˜ëŠ” ëª¨ë¸ë¡œ, 2017ë…„ NIPSì—ì„œ Googleì´ ì†Œê°œí•˜ì˜€ë‹¤.</p>

<p>RNNì—ì„œ ë²—ì–´ë‚˜ Attentionë§Œì„ ì‚¬ìš©í•˜ëŠ” ì‹ ê²½ë§ì„ ê³ ì•ˆí•˜ì—¬ Multi-head Self-attentionì„ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì´ë‹¤.</p>

<p>RNNì˜ ìˆœì°¨ì  ê³„ì‚°ì„ í–‰ë ¬ê³±ì„ ì‚¬ìš©í•˜ì—¬ í•œ ë²ˆì— ì²˜ë¦¬í•¨ìœ¼ë¡œì¨ ëª¨ë“  ì¤‘ìš” ì •ë³´ë¥¼ Embedding í•œë‹¤.</p>

<p>Self-attentionì„ í†µí•´ ê°™ì€ ë¬¸ì¥ ë‚´ ëª¨ë“  ë‹¨ì–´ ìŒ ì‚¬ì´ì˜ ì˜ë¯¸, ë¬¸ë²• ê´€ê³„ë¥¼ ì•Œ ìˆ˜ ìˆë‹¤.</p>

<p>ê·¸ëŸ¬ë‚˜ Positional Encodingë§Œìœ¼ë¡œëŠ” ìœ„ì¹˜, ìˆœì„œ ì •ë³´ ì œê³µì— ì–´ë ¤ì›€ì´ ìˆì–´, BERTê°€ ë“±ì¥í•˜ê²Œ ë˜ì—ˆë‹¤. BERTì—ì„œëŠ” Positional Embeddingì´ë‹¤.</p>

<h2 id="model">Model</h2>

<p>Inputs: Encoder / Outputs: Decoderë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤.</p>

<p><img src="https://user-images.githubusercontent.com/66259854/104466229-56b6c100-55f8-11eb-9662-1acfd8adf05c.png" alt="image" /></p>

<ul>
  <li>Input(Output) Embedding</li>
  <li>Positional Encoding</li>
  <li>(Encoder-Decoder) (Masked) Multi Head (Self) Attention</li>
  <li>Scaled Dot Product Attention</li>
  <li>Dropout</li>
  <li>Layer Normalization</li>
  <li>Sub-layer &amp; Residual Connection</li>
  <li>Feed Forward</li>
  <li>Linear &amp; Softmax</li>
</ul>

<hr />

<p><img src="https://user-images.githubusercontent.com/66259854/104466270-61715600-55f8-11eb-8a2a-ad56d09a66a0.png" alt="image" /></p>

<p>ë…¼ë¬¸ì—ì„œëŠ” Encoderì™€ Decoderë¥¼ 6ê°œì”© ìŒ“ì•„ Encoding ë¶€ë¶„ê³¼ Decoding ë¶€ë¶„ì„ ë§Œë“¤ì—ˆë‹¤.</p>

<h2 id="layer">Layer</h2>

<p>Dropoutì— ëŒ€í•œ ì„¤ëª…ì€ ìƒëµí•œë‹¤.</p>

<p>Self Attentionì´ ì•„ë‹ˆê±°ë‚˜, Output Embeddingì¸ ê²½ìš°ê°€ ìˆìœ¼ë‚˜ ìœ„ì—ì„œ í•œ ë²ˆ í‘œê¸°í•œ ì´í›„ë¡œ ìƒëµí•œë‹¤.</p>

<p>ë˜í•œ Tensorflowì™€ Pytorchê°€ í˜¼ì¬ë˜ì–´ ìˆë‹¤.</p>

<h3 id="input-embedding">Input Embedding</h3>

<p>Word Embedding</p>

<p><a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding">tf.keras.layers.Embedding TensorFlow Core v2.3.0</a></p>

<p><a href="https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca">Glossary of Deep Learning: Word Embedding</a></p>

<p>Convert 2D sequence (batch_size, input_length)</p>

<p>â†’ 3D (batch_size, input_length, $d_{model}$)</p>

<p>Word Embeddingì€ 6ê°œ ì¤‘ ê°€ì¥ ë°‘ì— ìˆëŠ” Encoderì—ì„œë§Œ ì¼ì–´ë‚œë‹¤.</p>

<h3 id="positional-encoding">Positional Encoding</h3>

<p><img src="https://user-images.githubusercontent.com/66259854/104466284-67673700-55f8-11eb-8399-a97d4b1da32f.png" alt="image" /></p>

<p>RNNì€ ë‹¨ì–´ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì…ë ¥ë°›ê¸° ë•Œë¬¸ì—, ë‹¨ì–´ì˜ ìœ„ì¹˜ì— ë”°ë¼ ìœ„ì¹˜ ì •ë³´ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆë‹¤.</p>

<p>ì´ íŠ¹ì§•ìœ¼ë¡œ ìì—°ì–´ ì²˜ë¦¬ì—ì„œ RNNì´ ìì£¼ ì‚¬ìš©ë˜ì—ˆë‹¤.</p>

<p>ê·¸ë¦¬ê³  Transformerì—ì„œëŠ” ìœ„ì¹˜ ì •ë³´ë¥¼ ìœ„í•´ Positional Encodingì„ ì‚¬ìš©í•œë‹¤.</p>

<hr />

\[PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})\]

\[PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})\]

<p>$2i$ = ì§ìˆ˜ / $2i+1$ = í™€ìˆ˜</p>

<p>pos = Embedding Vectorì˜ ìœ„ì¹˜. <del>Sequence Batch ìˆ˜.</del></p>

<p>index = Embedding Vector ë‚´ Dimension Index. <del>Embedding Dimension.</del></p>

<h3 id="multi-head-self-attention">Multi Head Self Attention</h3>

<p><img src="https://user-images.githubusercontent.com/66259854/104466694-db094400-55f8-11eb-98a6-55e0a2eb4ca3.png" alt="image" /></p>

<p>Multi Head Self Attentionì€ Query, Key, Value headë¡œ ë‚˜ë‰˜ê³ , ê°ê° ë‹¤ë¥¸ Linear Projection, Scaled Dot-Productë¥¼ ì§„í–‰í•œë‹¤. (Splitì„ ì§„í–‰í•˜ê¸° ë•Œë¬¸ì— ê°ê° ë‹¤ë¥¸ Scaled Dot-Productë¥¼ ì§„í–‰í•œë‹¤.)</p>

<p>ì´í›„ Concat, Linear Projectionì„ í•˜ëŠ”ë°, ì´ëŠ” tf.reshape, tf.transformìœ¼ë¡œ í•œ ë²ˆì— ì—°ì‚°í•  ìˆ˜ ìˆë‹¤.</p>

<h3 id="scaled-dot-product-attention">Scaled Dot Product Attention</h3>

<p><img src="https://user-images.githubusercontent.com/66259854/104466713-df356180-55f8-11eb-9e5c-5cd6faac972c.png" alt="image" /></p>

<p>Learnable Parameterê°€ ì—†ë‹¤.</p>

<p>ê¸°ì¡´ Additive Attentionì€ Attention scoreë¥¼ êµ¬í•˜ëŠ” êµ¬ê°„ì— Feed Forward Layerê°€ ìˆì§€ë§Œ, Dot Product ì—°ì‚°ìœ¼ë¡œ ëŒ€ì²´í•˜ì˜€ë‹¤.</p>

<p>Encoderì—ì„œ Paddingì„ ì‚¬ìš©í•˜ì§€ ì•Šë„ë¡ Padding Maskë¥¼ ì¶”ê°€í•´ì•¼ í•œë‹¤.</p>

<h3 id="sub-layer--residual-connection-layer-normalization">Sub-layer &amp; Residual Connection, Layer Normalization</h3>

<p><img src="https://user-images.githubusercontent.com/66259854/104466733-e492ac00-55f8-11eb-873e-e56a6bc92906.png" alt="image" /></p>

<p>Encoderì—ëŠ” 2ê°œì˜ Sub-layerê°€ ìˆê³ , Decoderì—ëŠ” 3ê°œì˜ Sub-layerê°€ ìˆë‹¤.</p>

<p>ì´ Sub-layerëŠ” Residual Connectionì„ ê±°ì¹œë‹¤.</p>

<hr />

<p><a href="https://arxiv.org/abs/1607.06450">Layer Normalization</a></p>

\[\bar{x} = \frac{a}{\sigma}(x - \mu) + b\]

\[LayerNorm(x_i)=\gamma \ \frac{x_{i, k} - \mu_i}{\sqrt{\sigma_i^2} + \epsilon} + \beta&amp;&amp;
&amp;&amp;(\gamma=1,\ \beta=0)\]

<p>ì´í›„ $LayerNorm(xÂ +Â Sublayer(x))$ ì´ ì‹ì²˜ëŸ¼ Layer Normalizationì„ ì ìš©í•œë‹¤.</p>

<p>LNì€ Tensorì˜ ë§ˆì§€ë§‰ ì°¨ì›ì— ëŒ€í•´ì„œ í‰êµ°ê³¼ ë¶„ì‚°ì„ êµ¬í•˜ê³  ìœ„ì˜ ìˆ˜ì‹ì„ í†µí•´ ê°’ì„ ì •ê·œí™”í•œë‹¤.</p>

<h3 id="feed-forward">Feed Forward</h3>

\[FFN(x)=max(0,xW_1+b_1)W_2+b_2\]

<p>Multi Head Attentionì—ì„œ ë‚˜ì˜¨ Attention ì •ë³´ë¥¼ ì •ë¦¬í•˜ëŠ” ì—­í• .</p>

<p><code class="highlighter-rouge">FF - Relu - Dropout - FF</code> ìˆœì„œì˜ Sequentialí•œ êµ¬ì¡°ì´ë‹¤.</p>

<h3 id="linear--softmax">Linear &amp; Softmax</h3>

<p><img src="https://user-images.githubusercontent.com/66259854/104466753-eb212380-55f8-11eb-9f12-2e00a64433b1.png" alt="image" /></p>

<ol>
  <li>
    <p>Linear Layer.</p>

    <p>Fully-connectedë¡œ ë§ˆì§€ë§‰ Decoder Outputì„ Logits Vectorì— íˆ¬ì˜ì‹œí‚¨ë‹¤.</p>

    <p>Logits VectorëŠ” Output Vocabularyë¡œ ë§ì€ ë‹¨ì–´ê°€ ë“¤ì–´ê°€ ìˆê³ , Vectorì˜ ê° ì…€ì€ ëŒ€ì‘í•˜ëŠ” ë‹¨ì–´ì˜ ì ìˆ˜ì´ë¯€ë¡œ ì¶œë ¥ì„ í•´ì„í•  ìˆ˜ ìˆë‹¤.</p>
  </li>
  <li>
    <p>Softmax Layer.</p>

    <p>ì´ ì ìˆ˜ë“¤ì„ Softmaxë¥¼ í†µí•´ í™•ë¥ ê°’ìœ¼ë¡œ ë³€í™˜í•œë‹¤.</p>

    <p>Argmaxë¥¼ ì´ìš©í•˜ì—¬ ë‚˜ì˜¨ ê°€ì¥ ë†’ì€ í™•ë¥ ê°’ì´ ìµœì¢… Outputì´ ëœë‹¤.</p>
  </li>
</ol>

<h2 id="encoder">Encoder</h2>

<p><code class="highlighter-rouge">- Multi Head Attention(Mask)</code></p>

<p><code class="highlighter-rouge">- Dropout1</code></p>

<p><code class="highlighter-rouge">- LayerNorm with Residual Connection1 â†’ Output</code></p>

<p><code class="highlighter-rouge">-</code></p>

<p><code class="highlighter-rouge">- Position wise Feed Forward</code></p>

<p><code class="highlighter-rouge">- Dropout2</code></p>

<p><code class="highlighter-rouge">- LayerNorm with Residual Connection2 â†’ Output</code></p>

<p>tf.add()ë¡œ ResNetì—ì„œ ì‚¬ìš©í•˜ëŠ” Residual Connectionì„ í•œë‹¤.</p>

<p>Paddingì„ ì ìš©í•˜ì§€ ëª»í•˜ê²Œ Padding Maskë¥¼ ë„£ëŠ”ë‹¤.</p>

<h2 id="decoder">Decoder</h2>

<p><code class="highlighter-rouge">- Masked Multi Head Attention</code></p>

<p><code class="highlighter-rouge">- Dropout1</code></p>

<p><code class="highlighter-rouge">- LayerNorm with Residual Connection1 â†’ Query</code></p>

<p><code class="highlighter-rouge">-</code></p>

<p><code class="highlighter-rouge">- Encoder-Decoder Multi Head Attention
{Input: Query, Key(Encoder Output), Value(Encoder Output)}</code></p>

<p><code class="highlighter-rouge">- Dropout2</code></p>

<p><code class="highlighter-rouge">- LayerNorm with Residual Connection2 â†’ En/Decoder Output</code></p>

<p><code class="highlighter-rouge">-</code></p>

<p><code class="highlighter-rouge">- Position wise Feed Forward</code></p>

<p><code class="highlighter-rouge">- Dropout3</code></p>

<p><code class="highlighter-rouge">- LayerNorm with Residual Connection3 â†’ Output</code></p>

<p>Encoderì˜ Outputì´ Key, Valueê°€ ëœë‹¤.</p>

<p>Masked Multi Head Attentionì—ëŠ” Look Ahead Maskì„ ë„£ê³ ,</p>

<p>Encoder-Decoder Multi Head Attentionì—ëŠ” ë˜ Padding Maskë¥¼ ë„£ëŠ”ë‹¤.</p>

<h2 id="multi-head-self-attention---scaled-dot-product-attention">Multi Head Self Attention &amp;  Scaled Dot Product Attention</h2>

<p>Layerì˜ Multi Head Self Attentionê³¼ Scaled Dot Product Attentionì„ ìƒì„¸íˆ ì„œìˆ í•œë‹¤.</p>

<h3 id="vector-calculation">Vector Calculation.</h3>

<p>ë¬¸ì¥ ë‚´ íŠ¹ì • ë‹¨ì–´ì— ëŒ€í•œ Self-attentionì„ ê³„ì‚°í•˜ë ¤ë©´, ë¬¸ì¥ì˜ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ê³¼ ê°ê° ì ìˆ˜ë¥¼ ê³„ì‚°í•´ì•¼ í•œë‹¤.</p>

<p>ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë¹ ë¥¸ ì†ë„ë¥¼ ìœ„í•´ Vectorê°€ ì•„ë‹Œ Matrixë¥¼ ì‚¬ìš©í•œë‹¤.</p>

<ol>
  <li>
    <p>Create Head Vector</p>

    <p><img src="https://user-images.githubusercontent.com/66259854/104466771-f07e6e00-55f8-11eb-9e0c-871e9fdeb7aa.png" alt="image" /></p>

    <p>3ê°€ì§€ Head VectorëŠ” Input Vectorì™€ 3ê°œì˜ í•™ìŠµ ê°€ëŠ¥í•œ í–‰ë ¬ì„ ê°ê° ê³±í•´ì„œ ë§Œë“¤ì–´ì§„ë‹¤.</p>

    <p>Input Vectorì˜ í¬ê¸°ê°€ $d_{model}=512$ì¼ ë•Œ, Head Vectorì˜ í¬ê¸°ëŠ” 64ì¸ë°, Attentionì˜ ê³„ì‚° ë³µì¡ë„ë¥¼ ì¼ì •í•˜ê²Œ ë§Œë“¤ê³ ì í•˜ëŠ” êµ¬ì¡° ë•Œë¬¸ì´ë‹¤.</p>

    <p>(ë˜í•œ 64ë¼ëŠ” ê°’ì€ $num-heads=8$ë¡œ ê²°ì •ë˜ëŠ”ë°, $d_{model}$ì„ $num-heads$ë¡œ ë‚˜ëˆˆ ê°’ì´ë‹¤.)</p>
  </li>
  <li>
    <p>Matmul</p>

    <p><img src="https://user-images.githubusercontent.com/66259854/104466792-f411f500-55f8-11eb-98a7-c705415e851c.png" alt="image" /></p>

    <p>í˜„ì¬ ë‹¨ì–´ì˜ Q Vectorì™€ ëª¨ë“  ë‹¨ì–´ì˜ K Vertorë¥¼ Matmul í•œë‹¤.</p>
  </li>
  <li>
    <p>Scale &amp; Softmax (Mask ê³¼ì •ì„ ê±°ì¹  ìˆ˜ ìˆë‹¤.)</p>

    <p><img src="https://user-images.githubusercontent.com/66259854/104466808-f83e1280-55f8-11eb-81b8-96cc4ab4b8b5.png" alt="image" /></p>

    <ol>
      <li>
        <p>ì ìˆ˜ë“¤ì„ 8ë¡œ ë‚˜ëˆ„ëŠ”ë°, Key Vectorì˜ í¬ê¸° 64ì˜ ì œê³±ê·¼ì´ë‹¤.</p>

        <p>ì´ë¥¼ Attention Scoreë¼ê³  ë¶€ë¥¸ë‹¤.</p>
      </li>
      <li>
        <p>Softmaxë¥¼ ì·¨í•˜ê³ , ì´ ì ìˆ˜ëŠ” í˜„ì¬ ìœ„ì¹˜ì˜ ë‹¨ì–´ì—ì„œ ê° ë‹¨ì–´ë“¤ì˜ í‘œí˜„ì´ ì–¼ë§ˆë‚˜ í¬í•¨ë˜ëŠ”ì§€ ê²°ì •í•œë‹¤.</p>

        <p>ë‹¹ì—°íˆ í˜„ì¬ ìœ„ì¹˜ì˜ ë‹¨ì–´ê°€ ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ë³´ì´ë‚˜, ë‹¤ë¥¸ ë‹¨ì–´ì— ëŒ€í•œ ì •ë³´ë„ í¬í•¨ë˜ì–´ ìˆë‹¤.</p>
      </li>
    </ol>
  </li>
  <li>
    <p>Matmul</p>

    <p><img src="https://user-images.githubusercontent.com/66259854/104466823-fbd19980-55f8-11eb-8eff-73760822a595.png" alt="image" /></p>

    <p>ì ìˆ˜ì— V Vectorë¥¼ ê³±í•œë‹¤.</p>

    <p>ì´ë¥¼ Attention Value í˜¹ì€ Context Vectorë¼ê³  ë¶€ë¥¸ë‹¤.</p>

    <p>Attentionì„ ìœ„í•´ ê´€ë ¨ì´ ìˆëŠ” ë‹¨ì–´ëŠ” ë‚¨ê²¨ë‘ê³ , ê´€ë ¨ì´ ì—†ëŠ” ë‹¨ì–´ëŠ” 0.001ê³¼ ê°™ì€ ì•„ì£¼ ì‘ì€ ìˆ«ìë¥¼ ê³±í•´ ì—†ì•¤ë‹¤.</p>
  </li>
  <li>
    <p>Concat</p>

    <p>ì ìˆ˜ì™€ V Vectorê°€ ê³±í•´ì§„ Attention Valueë¥¼ Concat í•œë‹¤.</p>

    <p>í˜„ì¬ ìœ„ì¹˜ì— ëŒ€í•œ Self Attentionì˜ ì¶œë ¥ì´ë‹¤.</p>
  </li>
</ol>

<h3 id="matrix-calcualtion">Matrix Calcualtion.</h3>
<ol>
  <li>
    <p>Query, Key, Value Matrix</p>

    <p><img src="https://user-images.githubusercontent.com/66259854/104466832-ff652080-55f8-11eb-9c85-7ed4d95ed7a1.png" alt="image" /></p>

    <p>Input Vector or Embedding Vectorë¥¼ í•˜ë‚˜ì˜ í–‰ë ¬ Xë¡œ ìŒ“ì•„ ì˜¬ë¦¬ê³ , í•™ìŠµí•  Weight í–‰ë ¬ì„ ê³±í•´ Q, K, Vë¥¼ ê³„ì‚°í•œë‹¤.</p>

    <p>í–‰ë ¬ Xì˜ ê° í–‰ì€ ì…ë ¥ ë¬¸ì¥ì˜ ê° ë‹¨ì–´ì— í•´ë‹¹í•œë‹¤.</p>
  </li>
  <li>
    <p>One Equation</p>

    <p><img src="https://user-images.githubusercontent.com/66259854/104466853-0429d480-55f9-11eb-83c7-73efc5047835.png" alt="image" /></p>

    <p>í–‰ë ¬ì„ ì‚¬ìš©í•˜ë©´ Vector Calculation 2~5ë¥¼ í•˜ë‚˜ì˜ ì‹ìœ¼ë¡œ ì••ì¶•í•  ìˆ˜ ìˆë‹¤.</p>
  </li>
</ol>

<h3 id="multi-head">Multi Head.</h3>

<p><img src="https://user-images.githubusercontent.com/66259854/104466868-07bd5b80-55f9-11eb-9245-14ad6a9c555c.png" alt="image" /></p>

<p>Hê°œì˜ Query, Key, Value Weight í–‰ë ¬ì„ ê°–ê³  ìˆë‹¤.</p>

<p>ë…¼ë¬¸ì—ì„œëŠ” $num-heads=8$ê°œì˜ Attention Headë¥¼ ê°–ëŠ”ë‹¤.</p>

<p>ê° Attention Headì—ì„œ Query, Key, ValueëŠ” ëœë¤ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì–´ í•™ìŠµëœë‹¤.</p>

<p><img src="https://user-images.githubusercontent.com/66259854/104466881-0b50e280-55f9-11eb-93f3-48fd582581e0.png" alt="image" /></p>

<p>Self Attention ê³¼ì •ì„ ê±°ì¹˜ë©´ 8ê°œ(ë…¼ë¬¸ ê¸°ì¤€)ì˜ Z í–‰ë ¬ì´ ë‚˜ì˜¨ë‹¤.</p>

<p>ê·¸ëŸ¬ë‚˜ Feed FowardëŠ” í•œ ìœ„ì¹˜ì— ëŒ€í•´ í•œ ê°œì˜ í–‰ë ¬ë§Œ ë°›ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¬¸ì œê°€ ë°œìƒí•œë‹¤.</p>

<p><img src="https://user-images.githubusercontent.com/66259854/104466916-1277f080-55f9-11eb-91f6-2f8ec0b1ab45.png" alt="image" /></p>

<p>ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ 8ê°œì˜ Z í–‰ë ¬ì„ ì´ì–´ ë¶™ì—¬ì„œ í•˜ë‚˜ì˜ í–‰ë ¬ì„ ë§Œë“¤ê³ , ë˜ ë‹¤ë¥¸ Weight í–‰ë ¬ì¸ $W_O$ì„ ê³±í•œë‹¤.</p>

<p><img src="https://user-images.githubusercontent.com/66259854/104466933-173ca480-55f9-11eb-9b9b-c811931eefb9.png" alt="image" /></p>

<p>ëª¨ë‘ ìš”ì•½í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ê·¸ë¦¼ì´ ëœë‹¤.</p>

<p>Multi HeadëŠ” Self Attentionì„ ë³‘ë ¬ì ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤.</p>

<h3 id="difference-of-attention">Difference of Attention.</h3>

<p><img src="https://user-images.githubusercontent.com/66259854/104466947-1b68c200-55f9-11eb-9659-cf072d28d071.png" alt="image" /></p>

<ol>
  <li>
    <p>Encoder Self Attentionì€ Encoderì—ì„œ ì´ë£¨ì–´ì§„ë‹¤.</p>
  </li>
  <li>
    <p>Masked Decoder Self Attentionì€ Decoderì—ì„œ ì´ë£¨ì–´ì§„ë‹¤.</p>
  </li>
  <li>
    <p>Encoder-Decoder Attentionë„ Decoderì—ì„œ ì´ë£¨ì–´ì§€ë‚˜, Self-attentionì´ ì•„ë‹ˆë‹¤.</p>

    <p>Query: Decoder Vector / Key, Value: Encoder Vectorë¡œ, Headì˜ ì¶œì²˜ê°€ ê°™ì§€ ì•Šìœ¼ë¯€ë¡œ ë™ì¼í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì´ë‹¤.</p>
  </li>
</ol>

<h2 id="mask">Mask</h2>

<h3 id="padding-mask">Padding Mask.</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
    <span class="s">"Compute 'Scaled Dot Product Attention'"</span>
    <span class="n">d_k</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> \
             <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>
    <span class="n">p_attn</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># (Dropout described below)
</span>    <span class="n">p_attn</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">p_attn</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">p_attn</span><span class="p">,</span> <span class="n">value</span><span class="p">),</span> <span class="n">p_attn</span>
    <span class="sb">``</span><span class="err">`</span>

    <span class="sb">``</span><span class="err">`</span><span class="n">python</span>
    <span class="k">def</span> <span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
    <span class="s">'''ì¤‘ëµ'''</span>
        <span class="n">logits</span> <span class="o">+=</span> <span class="p">(</span><span class="n">mask</span> <span class="o">*</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span> <span class="c1"># ì–´í…ì…˜ ìŠ¤ì½”ì–´ í–‰ë ¬ì¸ logitsì— mask*-1e9 ê°’ì„ ë”í•´ì£¼ê³  ìˆë‹¤.
</span>    <span class="s">'''ì¤‘ëµ'''</span>
</code></pre></div></div>

<p>ì…ë ¥ ë¬¸ì¥ì— <PAD> Tokenì´ ìˆì„ ë•Œ, ìœ ì‚¬ë„ë¥¼ êµ¬í•˜ì§€ ì•Šë„ë¡  Maskingì„ í•˜ì—¬ Attentionì—ì„œ ì œì™¸í•œë‹¤.</PAD></p>

<ol>
  <li>-1e9ì™€ ê°™ì€ ì‘ì€ ìŒìˆ˜ê°’ì„ ê³±í•œë‹¤.</li>
  <li>Softmaxë¥¼ ì§€ë‚˜ê¸° ì´ì „ì— ì‘ì€ ìŒìˆ˜ê°’ì´ ìˆìœ¼ë¯€ë¡œ, Softmaxë¥¼ ì§€ë‚˜ë©´ 0ì— ê°€ê¹Œì›Œì§„ë‹¤.</li>
</ol>

<h3 id="look-ahead-mask">Look-ahead Mask.</h3>

<p><img src="https://user-images.githubusercontent.com/66259854/104466965-202d7600-55f9-11eb-9ab9-195fbc75c5a5.png" alt="image" /></p>

<p>seq2seq Decoderì™€ ë‹¬ë¦¬, Transformer DecoderëŠ” ë¬¸ì¥ í–‰ë ¬ë¡œ ì…ë ¥ì„ í•œ ë²ˆì— ë°›ëŠ”ë‹¤.</p>

<p>ë”°ë¼ì„œ í˜„ì¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•  ë•Œ, ë¯¸ë˜ ì‹œì ì˜ ë‹¨ì–´ë„ ì°¸ê³ í•˜ëŠ” ì¼ì´ ë°œìƒí•  ìˆ˜ ìˆë‹¤.</p>

<p>ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ Decoderì˜ ì²« ë²ˆì§¸ ì„œë¸Œì¸µì—ì„œ Look-ahead Maskë¥¼ ì¶”ê°€í•œë‹¤.</p>

<hr />

<p><img src="https://user-images.githubusercontent.com/66259854/104466981-24f22a00-55f9-11eb-97e4-4bef39ae6da9.png" alt="image" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">subsequent_mask</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
        <span class="s">"Mask out subsequent positions."</span>
        <span class="n">attn_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
        <span class="n">subsequent_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">attn_shape</span><span class="p">),</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'uint8'</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">subsequent_mask</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="o">@</span><span class="nb">staticmethod</span>
        <span class="k">def</span> <span class="nf">make_std_mask</span><span class="p">(</span><span class="n">tgt</span><span class="p">,</span> <span class="n">pad</span><span class="p">):</span>
            <span class="s">"Create a mask to hide padding and future words."</span>
            <span class="n">tgt_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">tgt</span> <span class="o">!=</span> <span class="n">pad</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">tgt_mask</span> <span class="o">=</span> <span class="n">tgt_mask</span> <span class="o">&amp;</span> <span class="n">Variable</span><span class="p">(</span>
                <span class="n">subsequent_mask</span><span class="p">(</span><span class="n">tgt</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">tgt_mask</span><span class="o">.</span><span class="n">data</span><span class="p">))</span>
            <span class="k">return</span> <span class="n">tgt_mask</span>
</code></pre></div></div>

<ol>
  <li>Self-attentionì„ í†µí•´ Attention Score Matrixë¥¼ ì–»ëŠ”ë‹¤.</li>
  <li>ë‹¤ìŒ ê·¸ë¦¼ê³¼ ê°™ì´ Maskingí•˜ì—¬ ë¯¸ë˜ ì‹œì ì˜ ë‹¨ì–´ë¥¼ ì°¸ê³ í•˜ì§€ ëª»í•˜ë„ë¡ ë°”ê¾¼ë‹¤.</li>
</ol>

<h2>ğŸ¸</h2>

<h3 id="auto-regressive--teacher-forcing">Auto Regressive &amp; Teacher Forcing.</h3>

<p><a href="https://www.tensorflow.org/tutorials/text/transformer#training_and_checkpointing">Transformer model for language understanding TensorFlow Core</a></p>

<p><a href="https://blog.naver.com/just_nlp/222136930059">ê³ ì‘ ì¸ê°„ : ë„¤ì´ë²„ ë¸”ë¡œê·¸</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Batch</span><span class="p">:</span>
    <span class="s">"Object for holding a batch of data with mask during training."</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">trg</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">src</span> <span class="o">=</span> <span class="n">src</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">src_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">src</span> <span class="o">!=</span> <span class="n">pad</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">trg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">trg</span> <span class="o">=</span> <span class="n">trg</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">trg_y</span> <span class="o">=</span> <span class="n">trg</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">trg_mask</span> <span class="o">=</span> \
                <span class="bp">self</span><span class="o">.</span><span class="n">make_std_mask</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trg</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ntokens</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trg_</span> <span class="o">!=</span> <span class="n">pad</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
</code></pre></div></div>

<ol>
  <li>
    <p>Transformerì˜ DecoderëŠ” Generate í•  ë•Œ ë¬´ì¡°ê±´ 1ê°œì˜ Tokenì„ ìƒì„±í•˜ë„ë¡ í•™ìŠµë˜ì–´ ìˆë‹¤. Sequenceë¥¼ Generate í•˜ë ¤ë©´ Sequence ê¸¸ì´ë§Œí¼ Decoderë¥¼ ë°˜ë³µí•´ì„œ ì‹¤í–‰í•œë‹¤.</p>
  </li>
  <li>
    <p>Inference ìƒí™©ì—ì„œ í•™ìŠµì„ ìƒê°í•´ ë³´ì. í•™ìŠµ ë‹¨ê³„ì—ì„œë„ 1ê°œì”© ìƒì„±í•˜ë„ë¡ í•™ìŠµí•´ì•¼ í•œë‹¤. Source - Target ë‘ ë¬¸ì¥ì€ Pairë¡œ ì˜ ê°€ì§€ê³  ìˆë‹¤. Decoderë¥¼ í•™ìŠµí•  ë•ŒëŠ” Targetì—ì„œ ì²« ë²ˆì§¸ ê¸€ìë¥¼ ì•ˆë‹¤ê³  ìƒê°í•˜ê³  ë‘ ë²ˆì§¸ ê¸€ìë¥¼ í•™ìŠµí•œë‹¤. ì„¸ ë²ˆì§¸ ê¸€ìëŠ” ì•ì˜ ë‘ ê¸€ìë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•œë‹¤. ì´ë ‡ê²Œ í•œ ê¸€ìì”© ì˜ˆì¸¡í•˜ê³  Lossë¥¼ ì¸¡ì •í•˜ëŠ” ë°©ì‹ì„ Auto Regressiveë¼ê³  í•œë‹¤.</p>
  </li>
  <li>
    <p>â€œë‘ ë²ˆì§¸ í† í°ì´ ì˜ëª» ìƒì„±ë˜ì—ˆìœ¼ë©´ ì„¸ ë²ˆì§¸ í† í°ì„ ë§Œë“¤ê³  Lossë¥¼ ì¸¡ì •í•  ë•Œ ë¬¸ì œê°€ ìƒê¸°ëŠ”ê°€?â€ë¼ëŠ” ì˜ë¬¸ì´ ë“¤ ìˆ˜ ìˆë‹¤. í•™ìŠµ ë‹¨ê³„ì—ì„œ ì˜ëª» ìƒì„±ë˜ì–´ë„ ê·¸ ê°’ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ì›ë³¸ ë¬¸ì¥ì„ ë‹¤ì‹œ ë„£ëŠ”ë‹¤. ìƒì„±í•œ í† í°ì„ ì§ì ‘ ë„£ëŠ” Inferenceì™€ ì°¨ì´ë¥¼ ë³´ì¸ë‹¤. ì´ë ‡ê²Œ ì˜ëª» ìƒì„±ë˜ì–´ë„ ë‹¤ìŒ í† í°ì„ ë§Œë“¤ ë•Œ ì˜¬ë°”ë¥¸ ë°ì´í„°ë¥¼ ë„£ëŠ” ë°©ì‹ì„ Teacher Forcingì´ë¼ê³  í•œë‹¤.</p>
  </li>
  <li>
    <p>ë”°ë¼ì„œ, DecoderëŠ” End Token ì§ì „ê¹Œì§€ ìƒì„± ëª¨ë¸ì„ íƒœì›Œì„œ End Tokenì´ ì˜ ë§Œë“¤ì–´ì§€ëŠ”ì§€ í™•ì¸í•˜ë©´ í•œ Sequenceë¥¼ í•™ìŠµí•œ ê²ƒì´ë‹¤. RNNì—ì„œë„ í•˜ë‚˜ë¥¼ ë§Œë“¤ê³ , ê·¸ ë‹¤ìŒ í† í°ì„ ë§Œë“¤ê¸° ìœ„í•´ ì´ì „ì˜ Vectorë¥¼ ì „ë¶€ ì´ìš©í•˜ëŠ” ê²ƒì„ ìƒê°í•˜ë©´ ëœë‹¤.</p>
  </li>
</ol>

<p>Original Comments.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>transformer ì˜ decoder ëŠ” generate í• ë•Œ ë¬´ì¡°ê±´ '1ê°œì˜ token' ì„ ìƒì„±í•˜ê²Œ í•™ìŠµë˜ì–´ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¼ sequence ë¥¼ generate í• ë ¤ë©´ sequence ê¸¸ì´ ë§Œí¼ decoder ë¥¼ ë°˜ë³µí•´ì„œ ì‹¤í–‰í•´ì•¼í•©ë‹ˆë‹¤.
ìœ„ ìƒí™©ì€ inference ìƒí™©ì´ë¼ê³  í•´ë³´ë©´, ê·¸ëŸ¼ í•™ìŠµì€ ì–´ë–»ê²Œ ì´ë¤„ì§ˆê¹Œ ìƒê°í•´ë³´ì‹œë©´ ë©ë‹ˆë‹¤. í•™ìŠµí• ë•Œë„ 1ê°œì”© ìƒì„±í•˜ê²Œ í•™ìŠµí•´ì•¼í•˜ëŠ”ë°, source - target ë‘ ë¬¸ì¥ì˜ pair ëŠ” ìŒìœ¼ë¡œ ì˜ ë“¤ê³  ìˆê³ , ê·¸ëŸ¼ decoder ë¥¼ í•™ìŠµí• ë•ŒëŠ” targetì—ì„œ ë§¨ ì• ê¸€ìë§Œ ì•ˆë‹¤ ì¹˜ê³  ê·¸ ë‹¤ìŒ ê¸€ìë¥¼ í•™ìŠµí•˜ê³ , ê·¸ ë‹¤ìŒì—ëŠ” 3ë²ˆì§¸ ê¸€ìë¥¼ ë§Œë“¤ê¸° ìœ„í•´ ì•ì— ë‘ ê¸€ìë§Œ ì‚¬ìš©í•˜ê² ì£ ?
ì´ë ‡ê²Œ í•œ ê¸€ìì”© ì˜ˆì¸¡í•˜ê³  loss ë¥¼ ì¸¡ì •í•˜ëŠ” ë°©ì‹ì„ auto regressive ë°©ì‹ ì´ë¼ê³  í•©ë‹ˆë‹¤.

ê·¸ëŸ¼ ë‹¤ìŒ ì˜ë¬¸ì´ ë“¤ ìˆ˜ ìˆëŠ”ë°,
'ë§Œì•½ ì²« ë²ˆì§¸ í† í° ê¸°ë°˜ìœ¼ë¡œ ë‘ ë²ˆì§¸ í† í°ì„ ìƒì„±í–ˆëŠ”ë°, ì´ê²Œ ì˜ëª» ë§Œë“¤ì–´ì¡Œìœ¼ë©´ ì„¸ ë²ˆì§¸ í† í°ì„ ë§Œë“¤ê³  ë¡œìŠ¤ë¥¼ ì¸¡ì •í• ë•Œ ì˜ëª» ë§Œë“¤ì–´ì§„ ë‘ ë²ˆì§¸ í† í°ì„ ì‚¬ìš©í•˜ë©´ ë¬¸ì œê°€ ë˜ëŠ”ê²Œ ì•„ë‹ê¹Œ?' ë¼ëŠ” ì˜ë¬¸ì´ ìƒê¸¸ ìˆ˜ ìˆëŠ”ë°, í•™ìŠµ ë‹¨ê³„ì—ì„œëŠ” ì˜ëª» ìƒì„±í–ˆì–´ë„ ê·¸ ê°’ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ì›ë³¸ ë¬¸ì¥ì„ ë‹¤ì‹œ ë„£ìŠµë‹ˆë‹¤. ì¸í¼ëŸ°ìŠ¤ í• ë•Œì™€ ë‹¤ë¥¸ ì ì´ì£ (ì¸í¼ëŸ°ìŠ¤ëŠ” ìƒì„±í•œ í† í°ì„ ì§ì ‘ ë„£ìŒ.)
ì´ë ‡ê²Œ ì˜ëª» ë§Œë“¤ì—ˆì–´ë„ ë°”ë¡œ ë‹¤ìŒ í† í°ì„ ìƒì„±í• ë•Œ ë°ì´í„°ë¥¼ ì˜¬ë°”ë¥¸ ë°ì´í„°ë¥¼ ë„£ì–´ì£¼ëŠ” ë°©ì‹ì„ teacher forcing ì´ë¼ê³  í•©ë‹ˆë‹¤. (ê·¼ë° ìœ„ ë§í¬ ë²ˆì—­ì´ êµì‚¬ ê°•ì œ... ë¼ê³  ë˜ì–´ìˆë„¤ìš”.. ì—¬ëŸ¬ë¶„ ì˜ì–´ë¡œ ê°™ì´ ë³´ì„¸ìš”...)

ë”°ë¼ì„œ, decoder ëŠ” ë¬¸ì¥ì˜ ë§ˆì§€ë§‰ì„ ì•Œë¦¬ëŠ” í† í° ì§ì „ê¹Œì§€ ìƒì„± ëª¨ë¸ì„ íƒœì›Œì„œ ë§ˆì§€ë§‰ í† í°ì´ ì˜ ë§Œë“¤ì–´ì§€ëŠ”ì§€ í™•ì¸í•˜ë©´ í•œ ì‹œí€€ìŠ¤ë¥¼ ì „ë¶€ í•™ìŠµí•œê²Œ ë©ë‹ˆë‹¤. auto regressive í•˜ê²Œ ë§ì´ì£ .

RNN ì—ì„œë„ í•˜ë‚˜ë¥¼ ë§Œë“¤ê³ , ê·¸ ë‹¤ìŒ í† í°ì„ ë§Œë“¤ê¸° ìœ„í•´ ì§ì „ vector ë“¤ì„ ì „ë¶€ ì´ìš©í•˜ëŠ”ê²ƒì„ ìƒê°í•˜ì‹œë©´ ë©ë‹ˆë‹¤ :)
</code></pre></div></div>

<h2 id="ë§í¬">ë§í¬</h2>

<p><a href="https://www.notion.so/BERT-c61ed6193e85436c8916c641c9372188">BERT</a></p>

<p><a href="https://colab.research.google.com/drive/1cwhr9FD4Ogmr8zNieCRICGD5V9kuT_jv#scrollTo=11l-Oq-Mjfg7">Google Colaboratory</a></p>

<p><a href="https://github.com/soline013/transformer-tensorflow2.0/blob/master/transformer_implement_tf2_0.ipynb">soline013/transformer-tensorflow2.0</a></p>

<p><a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a></p>

<p><a href="https://wikidocs.net/31379">ìœ„í‚¤ë…ìŠ¤</a></p>

<p><a href="https://www.tensorflow.org/tutorials/text/transformer">Transformer model for language understanding TensorFlow Core</a></p>

<p><a href="https://nlpinkorean.github.io/illustrated-transformer/">The Illustrated Transformer</a></p>

<p><a href="https://tutorials.pytorch.kr/beginner/torchtext_translation_tutorial.html">TorchTextë¡œ ì–¸ì–´ ë²ˆì—­í•˜ê¸° - PyTorch Tutorials 1.6.0 documentation</a></p>

<p><a href="https://www.quantumdl.com/entry/11%EC%A3%BC%EC%B0%A82-Attention-is-All-You-Need-Transformer">11ì£¼ì°¨(2) - Attention is All You Need (Transformer)</a></p>
:ET